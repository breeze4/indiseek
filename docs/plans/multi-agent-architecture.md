# Multi-Agent Architecture (Tier 3, Phase 8)

## Problem

The single-agent loop produces 60-80% quality answers. The core issue is the **gravity well**: initial searches pull the agent into one subsystem, and all subsequent exploration stays there. Tier 1 scaffolding (question reiteration, CRITIC, exploration tracking) added noise without solving this — quality dropped to 55-65%.

The gravity well is structural. A single conversation history means early tool results dominate the context. The agent never "steps back" because it has no mechanism to start fresh on a different subsystem. Adding hints to "explore other areas" gets ignored because the accumulated context biases toward what's already been found.

## Research Findings

Key data points from SOTA multi-agent systems (2024-2026):

**Anthropic's multi-agent research system** (June 2025): Orchestrator-worker pattern with Claude Opus lead + Sonnet subagents achieved **90.2% improvement** over single-agent Opus on research tasks. Token usage alone explains **80% of performance variance**. 3-5 subagents spawned in parallel, each with its own context window. Context isolation is the key mechanism — each subagent explores independently without path dependency. Costs ~15x more tokens than single-agent, but viable because subagent contexts are small. Scaling rules: simple fact check = 1 agent, 3-10 tool calls; comparison = 2-4 subagents, 10-15 calls each; complex research = 10+ subagents.

**Microsoft Code Researcher** (May 2025): Analysis → Synthesis → Validation three-phase architecture. Structured context memory stores (action, result) pairs. Explores **10 files per trajectory** vs SWE-agent's 1.33. Achieved **58% crash resolution** vs SWE-agent's 37.5% on kBenchSyz (Linux kernel crashes). Multiple search paths explored simultaneously during Analysis.

**VeriMAP** (2025): Planner generates both subtask DAG and per-subtask verification functions. Planner-generated verification beats generic verification by +9.46% on BigCodeBench-Hard. Key insight: verification criteria generated by the planner (who has global context) are more effective than generic verifier checks.

**Multi-agent vs single-agent RAG** (2024-2025 meta-analysis): Multi-agent improves precision 10-20% on average. Better for high-context (>30K token) scenarios. Subagent pattern processes ~67% fewer tokens than skills pattern due to context isolation. Multi-agent overhead hurts for simple tasks (<3 tool calls).

**Magentic-One** (Microsoft, Nov 2024): Orchestrator with dual-ledger — Task Ledger (facts, guesses, plan) and Progress Ledger (current progress, task assignments). Inner/outer loop allows replanning when progress stalls. Model-agnostic.

**MA-RAG** (arXiv 2505.20096, 2025): Four agents — Planner, Step Definer, Extractor, QA Agent. Most relevant ablation data: **removing the Planner drops HotpotQA EM from 50.7 → 36.2** (-14 points). Removing the Extractor (noise filter) drops it from 50.7 → 43.4 (-7 points). This quantifies the value of planning and evidence filtering separately.

**PRISM** (arXiv 2510.14278): Three agents (Analyzer, Selector, Adder) implementing a precision-recall loop. 90.9% recall on HotpotQA. The Adder agent decides "do we have enough evidence?" — prevents both under-exploration and over-exploration.

**LangChain pattern comparison** (blog, 2025): Subagents pattern is optimal for our use case (multiple domains, parallelization, context isolation). Subagents use 67% fewer tokens than Skills pattern on multi-domain queries because each operates with only its relevant context.

Full research survey: `docs/research/multi-agent-architectures.md`

**Implication for Indiseek**: Our queries are complex research tasks (avg 14 iterations, 22 tool calls, 15K+ context by end). We're in the zone where multi-agent clearly wins. Context isolation addresses the gravity well directly. The cost analysis is favorable because researcher contexts stay small. The MA-RAG ablation data specifically validates that the Planner phase is worth +14 EM points — the single highest-impact component.

## Architecture: Four-Phase Pipeline

```
Question
   │
   ▼
┌─────────┐     Structured plan (sub-questions + subsystems)
│ PLANNER │────────────────────────────────────┐
└─────────┘                                    │
                                               ▼
┌───────────┐  ┌───────────┐  ┌───────────┐   Each gets ONE sub-question
│RESEARCHER │  │RESEARCHER │  │RESEARCHER │   + clean context
│ (sub-q 1) │  │ (sub-q 2) │  │ (sub-q 3) │   + tool access
└─────┬─────┘  └─────┬─────┘  └─────┬─────┘
      │              │              │
      └──────────────┼──────────────┘
                     │  Evidence bundles
                     ▼
              ┌─────────────┐
              │ SYNTHESIZER │  No tool access. Stronger model optional.
              └──────┬──────┘  Takes evidence, produces answer.
                     │
                     ▼
              ┌──────────┐
              │ VERIFIER  │  Tool access. Checks claims against code.
              └──────┬───┘
                     │
                     ▼
              Final answer (with verification status)
```

### Phase 1: Planner

**Role**: Decompose the question into sub-questions, each scoped to a distinct subsystem or concern.

**Input**: User question + repo map (same as current system prompt).

**Output**: Structured plan as a list of sub-questions, each with:
- The sub-question text
- The suspected subsystem/directory to explore
- Suggested first tool calls
- Verification hints (what the Verifier should check for this sub-question — per VeriMAP pattern)

**Model**: Gemini Flash. Single LLM call, no tools.

**Why this helps**: Forces the agent to think architecturally before searching. "How does CSS HMR work?" becomes:
1. "How are CSS file changes detected on the server?" → `src/node/server/`
2. "How does the update propagate through the module graph?" → `src/node/server/hmr.ts`
3. "How does the browser apply the CSS update?" → `src/client/`
4. "Are there different paths for CSS Modules vs regular CSS?" → `src/node/plugins/css.ts`

The planner's subsystem hints prevent gravity wells by giving each researcher a starting direction.

**Prompt structure**:
```
You are a codebase research planner. Given a question about a codebase and its
directory structure, decompose the question into 2-5 specific sub-questions.

For each sub-question:
1. State the sub-question clearly
2. Identify which part of the codebase likely contains the answer (directory/file)
3. Suggest 1-2 initial tool calls (search_code query or resolve_symbol target)

Think about the architecture: most features involve multiple subsystems
(e.g., server + client, detection + propagation + application). Make sure
your sub-questions cover different subsystems, not just different aspects
of the same code.

Output as JSON:
{
  "sub_questions": [
    {
      "question": "...",
      "target_area": "src/node/server/",
      "initial_actions": ["search_code('file watcher change detection', mode='semantic')"],
      "verification_hint": "Verify: which library does the file watcher use? What event triggers the change handler?"
    }
  ]
}
```

### Phase 2: Researchers (parallel)

**Role**: Execute tool calls to gather evidence for ONE sub-question. Each researcher gets a clean context with only its sub-question and the repo map.

**Input**: Sub-question + target area hint + repo map.

**Output**: Evidence bundle — list of (tool_call, result, relevance_note) tuples.

**Model**: Gemini Flash. Tool-calling loop, same tools as current (search_code, resolve_symbol, read_file, read_map).

**Budget**: 6-8 iterations per researcher (vs 14 for current single agent). Total tool calls across all researchers will be similar or higher, but each researcher's context stays focused.

**Why this helps**: Each researcher starts with a clean context anchored to one subsystem. Researcher 1 explores server-side HMR, Researcher 2 explores client-side application, Researcher 3 explores CSS Modules. They can't fall into each other's gravity wells because they don't share conversation history.

**Key difference from current loop**: No synthesis pressure. The researcher's job is purely to gather evidence. Its final output is a structured evidence bundle, not a prose answer. This means it can spend all its budget on exploration without worrying about wrapping up.

**Sufficiency check** (inspired by PRISM's Adder agent): Each researcher should assess whether it has enough evidence before exhausting its budget. If a researcher finds a definitive answer to its sub-question in 3 iterations, it should stop early rather than wasting remaining budget. The condensation prompt at the end should include a coverage assessment: "What aspects of this sub-question remain unanswered?"

**Evidence bundle format**:
```python
@dataclass
class EvidenceBundle:
    sub_question: str
    findings: list[Finding]
    coverage_note: str  # What was and wasn't found

@dataclass
class Finding:
    tool: str
    args: dict
    result_summary: str  # Condensed, not raw tool output
    relevant_code: str   # Key code snippets extracted
    file_path: str
    line_range: tuple[int, int] | None
```

### Phase 3: Synthesizer

**Role**: Take all evidence bundles and produce a coherent, well-structured answer with code citations.

**Input**: Original question + all evidence bundles.

**Output**: Prose answer with file:line citations.

**Model**: Gemini Flash (or optionally Gemini Pro/Thinking for complex queries). Single LLM call, NO tool access.

**Why a separate agent**: The synthesizer sees all evidence from all subsystems simultaneously. It doesn't have 30 turns of tool-call history polluting its context. It gets clean, condensed evidence and can focus entirely on writing a good answer.

**Prompt structure**:
```
You are a technical writer synthesizing research about a codebase. You have been
given a question and evidence gathered by multiple researchers, each focused on
a different aspect.

Question: {question}

Evidence from Researcher 1 (sub-question: "..."):
{evidence_bundle_1}

Evidence from Researcher 2 (sub-question: "..."):
{evidence_bundle_2}

...

Write a comprehensive answer that:
1. Covers all sub-questions with evidence
2. Cites specific file paths and line numbers
3. Notes any gaps where evidence was incomplete
4. Identifies distinct behavioral paths/variants
```

### Phase 4: Verifier

**Role**: Extract factual claims from the synthesized answer and verify each against the codebase.

**Input**: The synthesized answer + access to tools.

**Output**: Verification report + corrected answer if needed.

**Model**: Gemini Flash. Tool-calling loop, 4-6 iterations.

**Why this helps**: The `<link>` tag inaccuracy in the eval happened because the single agent misremembered code it had read 15 turns earlier. The verifier re-reads relevant code with the specific claim in mind. Each verification is a targeted check, not exploratory browsing.

**VeriMAP insight**: The Planner should generate verification hints alongside sub-questions — e.g., "For sub-question 3, verify: what mechanism does the client use to apply CSS updates — does it modify an existing element or create a new one?" This gives the Verifier targeted criteria rather than relying on generic claim extraction. VeriMAP showed planner-generated verification beats generic verification by +9.46%.

**Verification prompt**:
```
Review this answer about a codebase. Extract every factual claim that references
specific code (function names, file paths, behaviors, call relationships).

For each claim:
1. Use the available tools to verify it
2. Mark as VERIFIED, CORRECTED, or UNVERIFIABLE
3. If CORRECTED, provide the correct information

Answer to verify:
{synthesized_answer}
```

## Design Decisions

### Shared Memory vs Message Passing

**Decision: Structured message passing (evidence bundles).**

Each agent produces a typed output that the next agent consumes. No shared mutable state. This is simpler than a shared memory store and prevents agents from interfering with each other.

The evidence bundle is the key data structure. Researchers produce them, the synthesizer consumes them, the verifier reads the synthesized answer.

### Model Selection Per Agent

| Agent | Model | Reasoning |
|-------|-------|-----------|
| Planner | Flash | Simple task: decompose question. One call. |
| Researchers | Flash | Tool-calling is Flash's strength. Multiple calls. |
| Synthesizer | Flash (default) / Pro (optional) | Synthesis quality might benefit from a stronger model. Start with Flash, evaluate whether Pro improves quality enough to justify 10x cost. |
| Verifier | Flash | Targeted verification: read code, compare to claim. |

### Parallel vs Sequential Researchers

**Decision: Sequential initially, parallel later.**

Running researchers in parallel requires async infrastructure (asyncio, thread pool, or similar). Start with sequential execution — run each researcher one at a time. The quality improvement comes from context isolation, not parallelism. Parallelism is a latency optimization we can add later.

### When to Use Multi-Agent vs Single-Agent

Not every query needs the full pipeline. Simple queries ("where is function X defined?") should use the current single-agent loop.

**Heuristic for routing**:
- Questions with "how" or "why" → multi-agent
- Questions about a specific symbol → single-agent
- Questions longer than ~15 words → multi-agent
- Questions containing "end-to-end", "flow", "architecture" → multi-agent

This mirrors the adaptive planning idea from the Tier 1 research.

### Researcher Budget and Iteration Count

Current single-agent: 14 iterations, ~22 tool calls average.

Multi-agent budget:
- Planner: 1 LLM call (no iteration loop)
- Per researcher: 8 iterations max, expect 5-6 average → ~8-10 tool calls each
- Synthesizer: 1 LLM call
- Verifier: 6 iterations max → ~4-6 verification tool calls

For a 3-sub-question query: 1 + (3 × ~8) + 1 + ~5 = ~31 tool calls across ~28 LLM calls.
Current: ~22 tool calls across ~14 LLM calls.

Cost increase: ~2x in LLM calls, ~1.4x in tool calls. But each LLM call has a much shorter context (researcher contexts are ~3K tokens vs current ~15K+ at later iterations), so actual token cost may be similar.

### Evidence Condensation

Raw tool outputs are verbose (search results, full file reads). The evidence bundle needs condensation. Each researcher should summarize its findings, extracting:
- Key code snippets (not full files)
- Symbol definitions and relationships
- File paths and line ranges

The researcher's final turn should produce a condensed summary, not just dump raw tool outputs.

## Implementation Plan

### Step 1: Evidence Bundle Data Structures

Add dataclasses for the evidence bundle, finding, sub-question plan, and verification result. These are the contracts between agents.

File: `src/indiseek/agent/multi.py` (new file)

- [x] `SubQuestion` dataclass: question, target_area, initial_actions
- [x] `ResearchPlan` dataclass: original_question, sub_questions list
- [x] `Finding` dataclass: tool, args, result_summary, relevant_code, file_path, line_range
- [x] `EvidenceBundle` dataclass: sub_question, findings, coverage_note
- [x] `VerificationResult` dataclass: claim, status (verified/corrected/unverifiable), correction
- [x] `MultiAgentResult` dataclass: answer, evidence_bundles, verification_results, plan

### Step 2: Planner Agent

Single-call LLM agent that decomposes a question into sub-questions.

File: `src/indiseek/agent/multi.py`

- [x] `PlannerAgent` class with `plan(question, repo_map) -> ResearchPlan`
- [x] System prompt that instructs decomposition with subsystem awareness
- [x] JSON output parsing with fallback for malformed responses
- [x] Unit test: given a question and mock repo map, produces valid ResearchPlan

### Step 3: Researcher Agent

Tool-calling agent loop (reuse existing `_execute_tool` logic) scoped to one sub-question.

File: `src/indiseek/agent/multi.py`

- [x] `ResearcherAgent` class with `research(sub_question, repo_map, store, searcher) -> EvidenceBundle`
- [x] Reuses `AgentLoop._execute_tool` (extract to shared method or mixin)
- [x] Shorter budget: 8 iterations, synthesis at 7
- [x] Evidence condensation turn at end: asks LLM to summarize findings into structured format
- [x] Question reiteration per turn (carry over from single-agent, proven useful)
- [x] Unit test: given a sub-question and mock tools, produces valid EvidenceBundle

### Step 4: Synthesizer Agent

Single-call LLM agent that produces the answer from evidence bundles.

File: `src/indiseek/agent/multi.py`

- [x] `SynthesizerAgent` class with `synthesize(question, evidence_bundles) -> str`
- [x] System prompt that emphasizes cross-subsystem coherence and citation requirements
- [x] Accepts optional model override (for testing Pro vs Flash)
- [x] Unit test: given evidence bundles, produces coherent answer

### Step 5: Verifier Agent

Tool-calling agent loop that checks claims in the synthesized answer.

File: `src/indiseek/agent/multi.py`

- [x] `VerifierAgent` class with `verify(answer, store, searcher) -> list[VerificationResult]`
- [x] System prompt that extracts claims and verifies each with targeted tool calls
- [x] Short budget: 6 iterations
- [x] If corrections found, revises the answer
- [x] Unit test: given an answer with a deliberate inaccuracy, detects it

### Step 6: Orchestrator

Ties the four agents together into the full pipeline. Handles routing (multi-agent vs single-agent).

File: `src/indiseek/agent/multi.py`

- [x] `MultiAgentOrchestrator` class with `run(question) -> MultiAgentResult`
- [x] Routing heuristic: classify question as simple or complex
- [x] Sequential execution: planner → researchers → synthesizer → verifier
- [x] Progress callbacks for each phase (compatible with existing SSE streaming)
- [x] Fallback: if planner fails or produces 1 sub-question, use single-agent loop

### Step 7: Integration with API

Wire the orchestrator into the existing `/api/query` endpoint.

File: `src/indiseek/api/dashboard.py`

- [x] Add `mode` parameter to query endpoint: "auto" (default), "single", "multi"
- [x] "auto" uses routing heuristic, "single" forces current loop, "multi" forces multi-agent
- [x] Update SSE progress events to include phase information
- [x] Update `AgentResult` or response format to include multi-agent metadata

### Step 8: Eval and Tuning

Run the CSS HMR eval with both single-agent and multi-agent to measure improvement.

- [ ] Run eval with single-agent (baseline)
- [ ] Run eval with multi-agent
- [ ] Compare against reference answer
- [ ] Tune researcher budget, planner prompt, synthesizer model
- [ ] Add 2-3 more eval questions for broader coverage

## Cost Analysis

Gemini 2.0 Flash pricing (as of Feb 2026):
- Input: $0.10/1M tokens
- Output: $0.40/1M tokens

Anthropic found their multi-agent system costs ~15x more tokens than single-agent. But that's comparing Opus (expensive) with multiple Sonnet subagents. Our case is different: we're using Flash for everything, and the key cost factor is **context window growth**.

**Current single-agent query** (estimated from logs):
- System prompt: ~3K tokens (repo map)
- 14 iterations, context grows quadratically: iter 1 sends ~4K, iter 14 sends ~20K+
- Total input tokens across all iterations: ~150K (sum of growing contexts)
- Output: ~5K tokens total
- Cost: ~$0.02 per query

**Multi-agent query** (estimated):
- Planner: ~4K input, ~500 output (1 call)
- 3 researchers × 8 iterations each:
  - Context stays small (~3K system + grows to ~8K max per researcher)
  - Total per researcher: ~40K input, ~2K output
  - Total all researchers: ~120K input, ~6K output
- Synthesizer: ~10K input (question + condensed evidence), ~2K output (1 call)
- Verifier: 6 iterations, context ~5K → 10K
  - Total: ~45K input, ~2K output
- **Grand total: ~180K input, ~10K output**
- Cost: ~$0.02 per query

Context isolation makes multi-agent cost-competitive despite more LLM calls. Single-agent context grows to 20K+ tokens at later iterations (all history replayed each turn). Researchers stay under 8K because each starts fresh. The total token count is similar but distributed across more, smaller calls.

The real cost advantage comes if we parallelize researchers — wall clock time drops from ~60s (sequential) to ~20s (parallel), with identical token cost.

## Risks and Mitigations

**Risk: Planner decomposition quality.** If the planner produces bad sub-questions (too vague, missing subsystems), the whole pipeline suffers.
- Mitigation: Include 2-3 worked examples in the planner prompt. Validate output schema. Fallback to single-agent if planner produces <2 sub-questions.

**Risk: Evidence condensation loses detail.** If researchers over-summarize, the synthesizer may lack specifics.
- Mitigation: Include raw code snippets in findings, not just summaries. Cap at ~2K tokens per finding.

**Risk: Overhead for simple queries.** Multi-agent adds latency even when it's not needed.
- Mitigation: Routing heuristic. Simple queries skip straight to single-agent.

**Risk: Verifier finds issues but can't fix them.** If verification reveals inaccuracies but the correction is wrong.
- Mitigation: Only apply corrections where the tool result clearly contradicts the claim. Flag ambiguous cases as "unverified" rather than attempting correction.

## Post-Implementation Notes

- Steps 1–7 added `UsageStats` return values to all agent methods (plan, research, synthesize, verify now return `tuple[result, UsageStats]`). The 11 tests in `test_multi_agent.py` that call these methods were not updated at the time, causing 9 test failures. Fixed by unpacking the tuple in each test.
